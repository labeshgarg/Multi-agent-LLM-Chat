import autogen
import tempfile
from autogen.coding import DockerCommandLineCodeExecutor
import streamlit as st
import asyncio
from autogen import AssistantAgent, UserProxyAgent
st.set_page_config(page_title="Collaborative code generation and execution by LLMs", page_icon="ðŸ’»", layout="wide")
st.write("# Code_GPT")

class TrackableAssistantAgent(AssistantAgent):
    def _process_received_message(self, message, sender, silent):
        with st.chat_message(sender.name):
            output = message["name"] + ": " + message["content"]
            st.markdown(output)
        return super()._process_received_message(message, sender, silent)

class TrackableUserProxyAgent(UserProxyAgent):
    def _process_received_message(self, message, sender, silent):
        return super()._process_received_message(message, sender, silent)



BASE_URL="http://localhost:11434/v1"
config_list_codellama = [
    {
        'cache_seed': 41,
        'base_url': BASE_URL,
        'api_key': "fakekey",
        'model': "codellama",
    }
]
llm_config_codellama={
    "config_list": config_list_codellama,
}
config_list_llama = [
    {
        'cache_seed': 41,
        'base_url': BASE_URL,
        'api_key': "fakekey",
        'model': "llama2:13b",
    }
]
llm_config_llama={
    "config_list": config_list_llama,
}
# Create a temporary directory to store the code files.
temp_dir = tempfile.TemporaryDirectory()

# Create a Docker command line code executor.
executor = DockerCommandLineCodeExecutor(
    image="python:3.11.5-slim",  # Execute code using the given docker image name.
    timeout=10,  # Timeout for each code execution in seconds.
    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.
)

# Create an agent with code executor configuration that uses docker.
code_executor_agent = TrackableAssistantAgent(
    "code_executor_agent_docker",
    llm_config=False,  # Turn off LLM for this agent.
    code_execution_config={"executor": executor},  # Use the docker command line code executor
    system_message="""
                    You will not write or modify any code.
                    1. If you think code_writer_agent is going out of context or writing code for any other problem tell code_writer_agent to rectify it.
                    """,
    human_input_mode="NEVER",  # Always take human input for this agent for safety.
)
code_writer_system_message = """You are a helpful AI assistant.
Solve tasks using your python coding skill.
1. Write the complete code in a single code block within three single quotes(example:'''code generated by you with testcases''') format and use # to comment anything that is not a part of your code.
2. If any test cases are given import 'unittest' and make a class named test and write the full code with testcases so that it can be executed as .py file. 
3. Do not write "[PYTHON][/PYTHON][TESTS][/TESTS]" in your code.
4. Do not suggest incomplete code which requires users to modify.
5. If the result indicates there is an error, fix the error smartly by reviewing your system message and output the full code in a single code block with changes instead of partial code.
6. If code_executor_agent gives blank output generate your complete code again in a single code block.
7. Reply 'TERMINATE' when code execution by code_executor_agent gives correct code output.
"""

code_writer_agent = TrackableAssistantAgent(
    "code_writer_agent",
    system_message=code_writer_system_message,
    llm_config=llm_config_codellama,
    max_consecutive_auto_reply=5,
    code_execution_config=False,  # Turn off code execution for this agent.
)
code_debugging_agent=TrackableUserProxyAgent(
    "code_debugging_agent",
    system_message="""
                    1. You are a code workflow monitoring agent.
                    2. You will ask code_writer_agent to regenerate the full code in a single code block .
                    3. You will not generate any code at any instant. Just say code_witer_agent to give the entire code in a single block.
                    4. If the code execution gives OK as output reply 'TERMINATE' to end the workflow.
    """,
    llm_config=llm_config_llama,
    max_consecutive_auto_reply=5,
    human_input_mode="NEVER",
    code_execution_config=False,  # Turn off code execution for this agent.
)
groupchat = autogen.GroupChat(agents=[code_writer_agent, code_executor_agent, code_debugging_agent], messages=[], max_round=10, speaker_selection_method="round_robin")
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config_codellama and llm_config_llama)
user_input = st.chat_input("Write your problems to solve ...")
if user_input:
    st.markdown(user_input)

    code_debugging_agent.initiate_chat(
            manager,
            message=user_input
            )
